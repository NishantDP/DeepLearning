{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1gRj-x7h332N"
   },
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "The network we built in the previous part isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-bEg-Zz4Q7z"
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is straightforward to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks.\n",
    "\n",
    "Training multilayer networks is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $\\ell$. We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss.\n",
    "\n",
    "To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "**Note:** I'm glossing over a few details here that require some knowledge of vector calculus, but they aren't necessary to understand what's going on.\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "worDfYepJH6j"
   },
   "source": [
    "## Import Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFdhxHwr57Yn"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "yCtUH8paXqBQ",
    "outputId": "1a4c93cf-21a8-4574-d121-f238912d28e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "\t• TensorFlow version: 2.0.0\n",
      "\t• tf.keras version: 2.2.4-tf\n",
      "\t• GPU device not found. Running on CPU\n"
     ]
    }
   ],
   "source": [
    "print('Using:')\n",
    "print('\\t\\u2022 TensorFlow version:', tf.__version__)\n",
    "print('\\t\\u2022 tf.keras version:', tf.keras.__version__)\n",
    "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zQV8MLaJOjN"
   },
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "Att74swb7Ol0",
    "outputId": "a98f6ee1-9881-4d8d-8766-b8b00a2cb4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset mnist (11.06 MiB) to /root/tensorflow_datasets/mnist/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "training_set, dataset_info = tfds.load('mnist', split='train', as_supervised = True, with_info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IiSe5BPrJquE"
   },
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9r4EMOdT9pM3"
   },
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "num_training_examples = dataset_info.splits['train'].num_examples\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_batches = training_set.cache().shuffle(num_training_examples//4).batch(batch_size).map(normalize).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9SC4gnUJucy"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mo2DfMVvAdbd"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TCpaAlcKCDB"
   },
   "source": [
    "## Getting the Model Ready For Training\n",
    "\n",
    "Before we can train our model we need to set the parameters we are going to use to train it. We can configure our model for training using the `.compile` method. The main parameters we need to specify in the `.compile` method are:\n",
    "\n",
    "* **Optimizer:** The algorithm that we'll use to update the weights of our model during training. Throughout these lessons we will use the [`adam`](http://arxiv.org/abs/1412.6980) optimizer. Adam is an optimization of the stochastic gradient descent algorithm. For a full list of the optimizers available in `tf.keras` check out the [optimizers documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers#classes).\n",
    "\n",
    "\n",
    "* **Loss Function:** The loss function we are going to use during training to measure the difference between the true labels of the images in your dataset and the predictions made by your model. In this lesson we will use the `sparse_categorical_crossentropy` loss function. We use the `sparse_categorical_crossentropy` loss function when our dataset has labels that are integers, and the `categorical_crossentropy` loss function when our dataset has one-hot encoded labels. For a full list of the loss functions available in `tf.keras` check out the [losses documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses#classes).\n",
    "\n",
    "\n",
    "* **Metrics:** A list of metrics to be evaluated by the model during training. Throughout these lessons we will measure the `accuracy` of our model. The `accuracy` calculates how often our model's predictions match the true labels of the images in our dataset. For a full list of the metrics available in `tf.keras` check out the [metrics documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics#classes).\n",
    "\n",
    "These are the main parameters we are going to set throught these lesson. You can check out all the other configuration parameters in the [TensorFlow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYv3pv5-InR1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5CjYa8ES3OI"
   },
   "source": [
    "## Taking a Look at the Loss and Accuracy Before Training\n",
    "\n",
    "Before we train our model, let's take a look at how our model performs when it is just using random weights. Let's take a look at the `loss` and `accuracy` values when we pass a single batch of images to our un-trained model. To do this, we will use the `.evaluate(data, true_labels)` method. The `.evaluate(data, true_labels)` method compares the predicted output of our model on the given `data` with the given `true_labels` and returns the `loss` and `accuracy` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "u_7aijzvJQZ7",
    "outputId": "f66f355e-d030-4c30-e50c-7bba125a20cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 3ms/sample - loss: 2.3485 - accuracy: 0.0625\n",
      "\n",
      "Loss before training: 2.343\n",
      "Accuracy before training: 6.250%\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
    "\n",
    "print('\\nLoss before training: {:,.3f}'.format(loss))\n",
    "print('Accuracy before training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvsfbLEMZjZ5"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "Now let's train our model by using all the images in our training set. Some nomenclature, one pass through the entire dataset is called an *epoch*. To train our model for a given number of epochs we use the `.fit` method, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Z-CgmnKBZDjq",
    "outputId": "38ab455c-767a-4705-c172-9d7cc926c239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 21s 23ms/step - loss: 0.2790 - accuracy: 0.9200\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1176 - accuracy: 0.9652\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0834 - accuracy: 0.9747\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0637 - accuracy: 0.9805\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0491 - accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(training_batches, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFgG_WfUjCic"
   },
   "source": [
    "The `.fit` method returns a `History` object which contains a record of training accuracy and loss values at successive epochs, as well as validation accuracy and loss values when applicable. We will discuss the history object in a later lesson. \n",
    "\n",
    "With our model trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "ghr7z-SnctRw",
    "outputId": "8e946c9a-56b5-45f4-e79f-c6451ff8b7d5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 235,
       "width": 424
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    ps = model.predict(image_batch)\n",
    "    first_image = image_batch.numpy().squeeze()[0]\n",
    "  \n",
    "  \n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
    "ax1.axis('off')\n",
    "ax2.barh(np.arange(10), ps[0])\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(np.arange(10))\n",
    "ax2.set_title('Class Probability')\n",
    "ax2.set_xlim(0, 1.1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4WcPdR9jKMB"
   },
   "source": [
    "WOW!! Now our network is brilliant. It can accurately predict the digits in our images. Let's take a look again at the loss and accuracy values for a single batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "rFZKBfTgfPVy",
    "outputId": "b4d7816a-bbfa-4bb8-c453-82506029aeb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 62us/sample - loss: 0.0432 - accuracy: 0.9844\n",
      "\n",
      "Loss after training: 0.029\n",
      "Accuracy after training: 98.438%\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
    "\n",
    "print('\\nLoss after training: {:,.3f}'.format(loss))\n",
    "print('Accuracy after training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqREWxKKVwql"
   },
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "Let's now take a minute to see how TensorFlow calculates and keeps track of the gradients needed for backpropagation. TensorFlow provides a class that records automatic differentiation operations, called `tf.GradientTape`. Automatic differentiation, also known as algorithmic differentiation or simply “autodiff”, is a family of techniques used by computers for efficiently and accurately evaluating derivatives of numeric functions.\n",
    "\n",
    "`tf.GradientTape` works by keeping track of operations performed on tensors that are being \"watched\". By default `tf.GradientTape` will automatically \"watch\" any trainable variables, such as the weights in our model. Trainable variables are those that have `trainable=True`. When we create a model with `tf.keras`, all of the parameters are initialized with `trainable = True`. Any tensor can also be manually \"watched\" by invoking the watch method.\n",
    "\n",
    "\n",
    "Let's see a simple example. Let's take the following equation:\n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "\n",
    "The derivative of `y` with respect to `x` is given by:\n",
    "\n",
    "$$\n",
    "\\frac{d y}{d x} = 2x\n",
    "$$\n",
    "\n",
    "Now, let's use `tf.GradientTape` to calculate the derivative of a tensor `y` with respect to a tensor `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2-ktpx5dVU3O",
    "outputId": "d4a54fba-61eb-4419-e9d9-8162785ef09d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient calculated by tf.GradientTape:\n",
      " tf.Tensor(\n",
      "[[1.1966898  0.12552415]\n",
      " [0.29263484 0.9696375 ]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "True Gradient:\n",
      " tf.Tensor(\n",
      "[[1.1966898  0.12552415]\n",
      " [0.29263484 0.9696375 ]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "Maximum Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed so things are reproducible\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Create a random tensor\n",
    "x = tf.random.normal((2,2))\n",
    "\n",
    "# Calculate gradient\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x ** 2\n",
    "    \n",
    "dy_dx = g.gradient(y, x)\n",
    "\n",
    "# Calculate the actual gradient of y = x^2\n",
    "true_grad = 2 * x\n",
    "\n",
    "# Print the gradient calculated by tf.GradientTape\n",
    "print('Gradient calculated by tf.GradientTape:\\n', dy_dx)\n",
    "\n",
    "# Print the actual gradient of y = x^2\n",
    "print('\\nTrue Gradient:\\n', true_grad)\n",
    "\n",
    "# Print the maximum difference between true and calculated gradient\n",
    "print('\\nMaximum Difference:', np.abs(true_grad - dy_dx).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgLCJaooV5Un"
   },
   "source": [
    "The `tf.GradientTape` class keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor.\n",
    "\n",
    "To know more about `tf.GradientTape` and trainable variables check the following links\n",
    "\n",
    "* [Gradient Tape](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape)\n",
    "\n",
    "* [TensorFlow Variables](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Variable)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part 3 - Training Neural Networks (Solution).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
